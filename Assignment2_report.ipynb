{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Understanding\n",
    "## Assignment 2: named entity recognition and dependency parsing\n",
    "The following requests have been fulfilled:\n",
    "\n",
    "0. Evaluate spaCy NER on CoNLL 2003 data (provided)\n",
    "    - report token-level performance (per class and total)\n",
    "    - report CoNLL chunk-level performance (per class and total);\n",
    "1. Grouping of Entities.\n",
    "Write a function to group recognized named entities using `noun_chunks` method of [spaCy](https://spacy.io/usage/linguistic-features#noun-chunks). Analyze the groups in terms of most frequent combinations (i.e. NER types that go together). \n",
    "2. One of the possible post-processing steps is to fix segmentation errors.\n",
    "Write a function that extends the entity span to cover the full noun-compounds. Make use of `compound` dependency relation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rationale\n",
    "Python code has been written in order to exploit the vast functionality of `spaCy` functions. Detailed comments on the logic behind each function can be found on the `assignment2.py` file itself; here we will present a higher level explanation of the code.\n",
    "The code has the main purpose of exploring and working with named entity recognition and dependency parsing through `spaCy`.\n",
    "A large number of functions has been designed in order to explore the various possibilities of working with a corpus which tokenizes differently with respect to `spacy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `extract_tokens(corpus)`\n",
    "The function reconstructs the sentences of the input corpus as plain strings, without performing any filtering or manipulation of the sentences themselves. Therefore, in this case, also all the `-DOCSTART-` tokens have been kept.\n",
    "The function is a simple list comprehension iterating over the sentences of the corpus and joining the encountered tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output (partial):\n",
    "## [['-DOCSTART-'], ['SOCCER', '-', 'JAPAN', 'GET', 'LUCKY', 'WIN', ',', 'CHINA', 'IN', 'SURPRISE', 'DEFEAT', '.']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `extract_tokens_clean(corpus)`\n",
    "The function reconstructs the sentences of the input corpus as plain strings, but this time filtering for not needed sentences (e.g. by removing the `-DOCSTART-` tokens).\n",
    "The function is a simple list comprehension iterating over the previously created object through `extract_tokens(corpus)` and filtering out unnecessary sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output (partial):\n",
    "## [['SOCCER', '-', 'JAPAN', 'GET', 'LUCKY', 'WIN', ',', 'CHINA', 'IN', 'SURPRISE', 'DEFEAT', '.'], ['Nadim', 'Ladki']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `extract_tags(corpus)`\n",
    "The function extracts tokens and relative IOB tags from the input coNLL corpus. It does not filter nor perform manipulation.\n",
    "It is a simple list comprehension iterating over all sentences of the corpus and storing in tuples the tokens and IOB tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `extract_tags_clean(corpus)`\n",
    "The function extracts tokens and relative IOB tags from the input coNLL corpus, but this time performing filtering of the `-DOCSTART-` tokens.\n",
    "It is a simple list comprehension iterating over all sentences of the previously generated object through `extract_tags(corpus)` and filtering out unnecessary sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `clean_sents(corpus)`\n",
    "The function reconstructs the sentences in string type of a corpus in coNLL format by also accounting for the correct distribution of whitespaces and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output (partial):\n",
    "## [['SOCCER - JAPAN GET LUCKY WIN, CHINA IN SURPRISE DEFEAT.'], ['Nadim Ladki']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `spacy_on_cleansents_text(corpus)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output (partial):\n",
    "## [['SOCCER', '-', 'JAPAN', 'GET', 'LUCKY', 'WIN', ',', 'CHINA', 'IN', 'SURPRISE', 'DEFEAT', '.'], ['Nadim', 'Ladki']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `spacy_on_cleansents_token(corpus)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output (partial):\n",
    "## [[SOCCER, -, JAPAN, GET, LUCKY, WIN, ,, CHINA, IN, SURPRISE, DEFEAT, .], [Nadim, Ladki]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `get_whitespaces(corpus)`\n",
    "The function retrieves the `token.whitespace_` attribute of each token and stores it into a list for later use.\n",
    "It is a simple list comprehension iterating over the sentences obtained through the `spacy_on_cleansents_token(corpus)` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output (partial):\n",
    "## [[True, True, True, True, True, False, True, True, True, True, False, False], [True, False]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `get_spacy_tags(corpus)`\n",
    "The function reconstructs the entities of the tokens of the input corpus without further manipulation.\n",
    "It is a list comprehension iterating over the sentences of the object produced by the function `clean_sents(corpus)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output (partial):\n",
    "## [[('SOCCER', 'O-'), ('-', 'O-'), ('JAPAN', 'O-'), ('GET', 'O-'), ('LUCKY', 'O-'), ('WIN', 'O-'), (',', 'O-'), ('CHINA', 'B-LOC'), ('IN', 'O-'), ('SURPRISE', 'O-'), ('DEFEAT', 'O-'), ('.', 'O-')], [('Nadim', 'B-ORG'), ('Ladki', 'I-ORG')]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `get_spacy_tags_clean(corpus)`\n",
    "The function reconstructs the entities of the tokens of the input corpus applying some manipulations.\n",
    "It is a list comprehension iterating over the sentences obtained through the function `get_spacy_tags(corpus)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output (partial):\n",
    "## [[('SOCCER', 'O'), ('-', 'O'), ('JAPAN', 'O'), ('GET', 'O'), ('LUCKY', 'O'), ('WIN', 'O'), (',', 'O'), ('CHINA', 'B-LOC'), ('IN', 'O'), ('SURPRISE', 'O'), ('DEFEAT', 'O'), ('.', 'O')], [('Nadim', 'B-ORG'), ('Ladki', 'I-ORG')]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `get_spacy_alignment(corpus)`\n",
    "The function returns the tokenization alignment as given by spacy. It is useful to check which sentences are differently tokenized between the coNLL corpus and the spacy corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `collapse_tokens(corpus)`\n",
    "The function reconstructs the tokenization done by spacy on the coNLL corpus in order to align the two. It does so by exploiting the whitespace information of the tokens and by manipulating the entity tags with respect to the ones given by spacy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `collapse_tokens_alternative(corpus)`\n",
    "The function reconstructs the tokenization done by spacy on the coNLL corpus, approaching the problem from a different perspective. It has been empirically noticed that by using this function over the previous one, the accuracy scores decrease substantially; it is therefore not used to the ends of the assignment, but it has been explored as a possibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `get_stats(corpus)`\n",
    "The function computes the chunk-level performance of the tokenization on the coNLL corpus by using the `conll.evaluate(ref, hyp)` function provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `accuracy_token_level(corpus)`\n",
    "The function computes the overall token-level accuracy of the tokenization on the coNLL corpus by using the `classification_report(tot, pos)` function of sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `get_chunks_ent(corpus)`\n",
    "The function performs grouping of noun chunk entities and then calculates the overall number of chunks which display the seen combinations of entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `span_compound(doc)`\n",
    "The function fixes segmentation errors (if present) by expanding the span of the compound token, through manipulation of its IOB tag. We analyze the cases in which the token is located in the proximity of the beginning or the end of an entity, in order to extend the span according to the `compound` dependency relation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
